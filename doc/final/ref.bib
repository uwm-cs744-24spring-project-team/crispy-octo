%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Mondo at 2024-05-06 23:08:29 -0500 


%% Saved with string encoding Unicode (UTF-8) 

@misc{sparkexamples,
	author = {Apache Spark},
	date-added = {2024-05-06 23:07:54 -0500},
	date-modified = {2024-05-06 23:08:24 -0500},
	howpublished = {\url{https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples}},
	title = {Spark Examples}
}

@misc{looksbusy,
	author = {flow2000},
	date-added = {2024-05-06 23:07:54 -0500},
	date-modified = {2024-05-06 23:08:24 -0500},
	howpublished = {\url{https://github.com/flow2000/lookbusy}},
	title = {Looksbusy}
}


@misc{kcontroller,
	author = {Kubernetes},
	date-added = {2024-05-06 23:07:54 -0500},
	date-modified = {2024-05-06 23:08:24 -0500},
	howpublished = {\url{https://kubernetes.io/docs/concepts/architecture/controller/}},
	title = {Kubernetes Controller}}

@misc{terraform,
        author = {terraform},
        howpublished = {https://www.terraform.io/},
        title = {Terraform}
}

@misc{volcano,
    author = {volcano},
    howpublished = 
    {https://volcano.sh/en/},
    title = {volcano}

}

@misc{:kube-batch,
	author = {Kubernetes},
	date-added = {2024-03-05 21:49:25 -0600},
	date-modified = {2024-03-05 21:50:19 -0600},
	howpublished = {\url{https://github.com/kubernetes-retired/kube-batch}},
	lastchecked = {Tuesday, February 27, 2024},
	title = {kube-batch: a batch scheduler for Kubernetes},
	bdsk-url-1 = {https://www.intel.com.br/content/dam/www/public/us/en/documents/white-papers/Intel_ByteDance_WhitePaper.pdf}}

@inproceedings{8891040,
	author = {Thinakaran, Prashanth and Gunasekaran, Jashwant Raj and Sharma, Bikash and Kandemir, Mahmut Taylan and Das, Chita R.},
	booktitle = {2019 IEEE International Conference on Cluster Computing (CLUSTER)},
	date-added = {2024-03-05 21:49:06 -0600},
	date-modified = {2024-03-05 21:49:06 -0600},
	doi = {10.1109/CLUSTER.2019.8891040},
	keywords = {Graphics processing units;Containers;Measurement;Task analysis;Quality of service;Resource management;Correlation},
	pages = {1-13},
	title = {Kube-Knots: Resource Harvesting through Dynamic Container Orchestration in GPU-based Datacenters},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/CLUSTER.2019.8891040}}

@inproceedings{romero2021infaas,
	author = {Romero, Francisco and Li, Qian and Yadwadkar, Neeraja J and Kozyrakis, Christos},
	booktitle = {2021 USENIX Annual Technical Conference (USENIX ATC 21)},
	date-added = {2024-03-05 20:22:34 -0600},
	date-modified = {2024-03-05 20:22:34 -0600},
	pages = {397--411},
	title = {$\{$INFaaS$\}$: Automated model-less inference serving},
	year = {2021}}

@inproceedings{hindman2011mesos,
	author = {Hindman, Benjamin and Konwinski, Andy and Zaharia, Matei and Ghodsi, Ali and Joseph, Anthony D and Katz, Randy and Shenker, Scott and Stoica, Ion},
	booktitle = {8th USENIX Symposium on Networked Systems Design and Implementation (NSDI 11)},
	date-added = {2024-03-05 20:19:18 -0600},
	date-modified = {2024-03-05 20:19:18 -0600},
	title = {Mesos: A platform for $\{$Fine-Grained$\}$ resource sharing in the data center},
	year = {2011}}

@misc{koo,
	author = {Koordinator Team},
	date-added = {2024-02-27 11:35:28 -0600},
	date-modified = {2024-02-27 11:36:16 -0600},
	howpublished = {\url{https://koordinator.sh}},
	title = {Koordinator, QoS-based scheduling for efficient orchestration of microservices, AI, and big data workloads on Kubernetes}}

@misc{:aa,
	author = {ByteDance, Intel},
	date-added = {2024-02-27 11:04:27 -0600},
	date-modified = {2024-02-27 11:36:28 -0600},
	howpublished = {\url{https://www.intel.com.br/content/dam/www/public/us/en/documents/white-papers/Intel_ByteDance_WhitePaper.pdf}},
	lastchecked = {Tuesday, February 27, 2024},
	title = {ByteDance: Performance Evaluation and Optimization for Workload Colocation},
	bdsk-url-1 = {https://www.intel.com.br/content/dam/www/public/us/en/documents/white-papers/Intel_ByteDance_WhitePaper.pdf}}


@misc{jeon_analysis_2019,
	title = {Analysis of {Large}-{Scale} {Multi}-{Tenant} {GPU} {Clusters} for {DNN} {Training} {Workloads}},
	url = {http://arxiv.org/abs/1901.05758},
	doi = {10.48550/arXiv.1901.05758},
	abstract = {With widespread advances in machine learning, a number of large enterprises are beginning to incorporate machine learning models across a number of products. These models are typically trained on shared, multi-tenant GPU clusters. Similar to existing cluster computing workloads, scheduling frameworks aim to provide features like high efficiency, resource isolation, fair sharing across users, etc. However Deep Neural Network (DNN) based workloads, predominantly trained on GPUs, differ in two significant ways from traditional big data analytics workloads. First, from a cluster utilization perspective, GPUs represent a monolithic resource that cannot be shared at a fine granularity across users. Second, from a workload perspective, deep learning frameworks require gang scheduling reducing the flexibility of scheduling and making the jobs themselves inelastic to failures at runtime. In this paper we present a detailed workload characterization of a two-month long trace from a multi-tenant GPU cluster in a large enterprise. By correlating scheduler logs with logs from individual jobs, we study three distinct issues that affect cluster utilization for DNN training workloads on multi-tenant clusters: (1) the effect of gang scheduling and locality constraints on queuing, (2) the effect of locality on GPU utilization, and (3) failures during training. Based on our experience running a large-scale operation, we provide design guidelines pertaining to next-generation cluster schedulers for DNN training workloads.},
	urldate = {2024-05-07},
	publisher = {arXiv},
	author = {Jeon, Myeongjae and Venkataraman, Shivaram and Phanishayee, Amar and Qian, Junjie and Xiao, Wencong and Yang, Fan},
	month = aug,
	year = {2019},
	note = {arXiv:1901.05758 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {arXiv Fulltext PDF:/Users/zihaozhu/Zotero/storage/53KAW5JP/Jeon et al. - 2019 - Analysis of Large-Scale Multi-Tenant GPU Clusters .pdf:application/pdf;arXiv.org Snapshot:/Users/zihaozhu/Zotero/storage/8YLULH7V/1901.html:text/html},
}

@misc{noauthor_trends_2023,
	title = {Trends for {DevOps} engineering in 2023},
	url = {https://www.traceroute42.com/blog/post?name=trends-for-devops-engineering-in-2023},
	abstract = {Guest post originally published on Traceroute42’s blog DevOps movement is still an evolving field and is influenced by a variety of factors such as technological advances, industry trends…},
	language = {en-US},
	urldate = {2024-05-07},
	journal = {CNCF},
	month = jan,
	year = {2023},
	file = {Snapshot:/Users/zihaozhu/Zotero/storage/YDD83B6X/trends-for-devops-engineering-in-2023.html:text/html},
}

@inproceedings{narayanan_heterogeneity-aware_2020,
	title = {\{{Heterogeneity}-{Aware}\} {Cluster} {Scheduling} {Policies} for {Deep} {Learning} {Workloads}},
	isbn = {978-1-939133-19-9},
	url = {https://www.usenix.org/conference/osdi20/presentation/narayanan-deepak},
	language = {en},
	urldate = {2024-05-07},
	author = {Narayanan, Deepak and Santhanam, Keshav and Kazhamiaka, Fiodar and Phanishayee, Amar and Zaharia, Matei},
	year = {2020},
	pages = {481--498},
	file = {Full Text PDF:C\:\\Users\\zzhu4\\Zotero\\storage\\I9GWLP2S\\Narayanan et al. - 2020 - Heterogeneity-Aware Cluster Scheduling Policies .pdf:application/pdf},
}

@article{ghodsi_dominant_nodate,
	title = {Dominant {Resource} {Fairness}: {Fair} {Allocation} of {Multiple} {Resource} {Types}},
	abstract = {We consider the problem of fair resource allocation in a system containing different resource types, where each user may have different demands for each resource. To address this problem, we propose Dominant Resource Fairness (DRF), a generalization of max-min fairness to multiple resource types. We show that DRF, unlike other possible policies, satisﬁes several highly desirable properties. First, DRF incentivizes users to share resources, by ensuring that no user is better off if resources are equally partitioned among them. Second, DRF is strategy-proof, as a user cannot increase her allocation by lying about her requirements. Third, DRF is envyfree, as no user would want to trade her allocation with that of another user. Finally, DRF allocations are Pareto efﬁcient, as it is not possible to improve the allocation of a user without decreasing the allocation of another user. We have implemented DRF in the Mesos cluster resource manager, and show that it leads to better throughput and fairness than the slot-based fair sharing schemes in current cluster schedulers.},
	language = {en},
	author = {Ghodsi, Ali and Zaharia, Matei and Hindman, Benjamin and Konwinski, Andy and Shenker, Scott and Stoica, Ion},
	file = {Ghodsi et al. - Dominant Resource Fairness Fair Allocation of Mul.pdf:C\:\\Users\\zzhu4\\Zotero\\storage\\F5RQFVFF\\Ghodsi et al. - Dominant Resource Fairness Fair Allocation of Mul.pdf:application/pdf},
}



